{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4385ea0-a993-484b-b00f-f3e48f89be2d",
   "metadata": {},
   "source": [
    "# NLP\n",
    "A field of AI that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret and generate human languages in a way that is both meaningful and useful.\n",
    "\n",
    "## Applications of NLP\n",
    "- Search Engines\n",
    "- Chatbot\n",
    "- Language Translation\n",
    "- Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ef9fa-c982-440d-b9d9-5e3ab80a20b8",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "- `.` - matches any charecter except a newline\n",
    "- `\\w` - matches any word charecter(alphanumaric-equivalent to `[a-zA-Z0-9_]`)\n",
    "- `\\d` - matches any digit(`[0-9])\n",
    "- `\\s` - matches any whitespace character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2671a-b54c-4b4c-8c90-f05a1cf13065",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bb705-a067-4d48-8147-9d9c46ee9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca622faa-93f0-4b80-8113-c6dfbc6c7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4f41a-2d86-4976-bf3a-eefab87282fc",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "It involves splitting text into smaller units, known as tokens. This token can be phrases, sentences or other meaningful units, depending on the granularity of the tokenization.\n",
    "## Types\n",
    "- Word Tokenization\n",
    "- Sentence Tokenization\n",
    "- Subword Tokenization\n",
    "- Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b509cb-ea28-4685-a414-9243a11bd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sentence=\"The quick brown fox jumps over the lazy dog. It was a sunny day\"\n",
    "\n",
    "words=word_tokenize(sentence)\n",
    "sentences=sent_tokenize(sentence)\n",
    "print(\"Word Tokens: \",end=\"\")\n",
    "print(words)\n",
    "print(\"Sentence Tokens: \",end=\"\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf831af1-37dc-4eaf-a1f3-280cfd287fd1",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "A text normalization technique used to reduce words to their base/root form. It simplify text data by reducing derived words to a common base form so that they can be analyzed as a single item.\n",
    "\n",
    "Stemming algorithms typically remove common word suffixes(int, ly, ed) to transform a word into its root form.\n",
    "\n",
    "__Example:__ `running` -> `run`, `better` -> `bet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28487e-c9e0-4854-bde8-2a7ed2caef02",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439f44f-1758-4581-866f-d9aa03b95a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3899a-3653-4824-9701-0b03faee0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4423102-d538-4144-8d9c-c379a3c0c421",
   "metadata": {},
   "source": [
    "## SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9576bf-997d-4f59-a2aa-2684cadf4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball=SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(f\"{word}->{snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43410db-306f-462a-9c23-d578bb94fa4f",
   "metadata": {},
   "source": [
    "## LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9c2d7-6dea-4d1d-be47-60ff5b9b0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word}->{lancaster.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e68160-7516-4942-a046-ecb32798afa0",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "A text normalization technique used to reduce words to their base form but unlike stemming, it considers the context and morphological analysis of words, aiming to reduce words to their meaningful root forms.\n",
    "\n",
    "__Example:__ `running` -> `run`, `better` -> `good`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581f508-f37b-4df8-801d-820c43938960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322a017-5bb9-4650-8a4b-443db616be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eae4e2-4b69-410e-8e05-cfa09949a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
    "    print(f\"{word}->{lancaster.stem(lemmatized_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa020a0-d4a7-40d7-baeb-6e9b7bc2a0a7",
   "metadata": {},
   "source": [
    "# Parts Of Speech Tagging\n",
    "It involves assigning parts of speech to each word in a sentence or text.\n",
    "## Tags\n",
    "- `NN` - Noun\n",
    "- `VB` - Verb\n",
    "- `JJ` - Adjective\n",
    "- `RB` - Adverb\n",
    "- `PRP` - Pronoun\n",
    "- `IN` - Preposition\n",
    "- `CC` - Conjunction\n",
    "- `DT` - Determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb452ab-35dc-4346-9021-94e06de3948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "for word in words:\n",
    "    print(f\"{word} -> {pos_tag([word])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bada8-de3e-4270-be91-d1100072c8b0",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "It involves identifying and `classifying` named entities in text into `predefined categories` such as persons, organizations, locations, dates and more.\n",
    "## Categories of Named Entities\n",
    "- `PER` - Person\n",
    "- `ORG` - Oragnization\n",
    "- `LOC` - Location\n",
    "- `DATE/TIME` - Date/Time\n",
    "- `MONEY` - Monetary Values\n",
    "- `PERCENT` - Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786b0f6-830f-40b3-aeb8-85c863aa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6fa613d-ee57-4b5c-a7c3-a0173f9c4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792b8622-7dbf-4046-8494-a5dcd3cc6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4576dcef-f9e8-4bc2-8c9f-e7d06c7c9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "Barack Obama PERSON\n",
      "August 4, 1961 DATE\n",
      "Honolulu GPE\n",
      "Hawaii GPE\n"
     ]
    }
   ],
   "source": [
    "# doc=nlp(sentence)\n",
    "doc=nlp(\"Apple is looking at buying U.K. startup for $1 billion. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae5158-1c42-49f7-9d89-4379c20dcf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
