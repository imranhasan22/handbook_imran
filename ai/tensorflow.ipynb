{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092985e2-f810-4180-ac8d-7b829e0d31a7",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "A tensor is a multi-dimensional array (like a matrix) and a fundamental data structure in TensorFlow. It’s designed to handle high-dimensional data and operations on that data, much like vectors and matrices in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7892c-5df7-4d95-9993-3499059effc8",
   "metadata": {},
   "source": [
    "## Ranks\n",
    "Rank refers to the number of dimensions in a tensor. A scalar (single value) has rank 0, a vector (1D array) has rank 1, a matrix (2D array) has rank 2, and so on.\n",
    "\n",
    "__Example of ranks:__\n",
    "- Rank 0 (Scalar): 1\n",
    "- Rank 1 (Vector): [1, 2, 3]\n",
    "- Rank 2 (Matrix): [[1, 2, 3], [4, 5, 6]]\n",
    "- Rank 3 (3D Tensor): [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4bd71e-0bd2-4a70-bdb8-08d6b4ba840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar: tf.Tensor(5, shape=(), dtype=int32)\n",
      "Vector: tf.Tensor([5 6 7], shape=(3,), dtype=int32)\n",
      "Matrix: tf.Tensor(\n",
      "[[ 5  6  7]\n",
      " [ 8  9 10]], shape=(2, 3), dtype=int32)\n",
      "3D Tensor: tf.Tensor(\n",
      "[[[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]], shape=(2, 2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Rank 0 tensor (scalar)\n",
    "scalar = tf.constant(5)\n",
    "\n",
    "# Rank 1 tensor (vector)\n",
    "vector = tf.constant([5, 6, 7])\n",
    "\n",
    "# Rank 2 tensor (matrix)\n",
    "matrix = tf.constant([[5, 6, 7], [8, 9, 10]])\n",
    "\n",
    "# Rank 3 tensor\n",
    "tensor_3d = tf.constant([[[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Vector:\", vector)\n",
    "print(\"Matrix:\", matrix)\n",
    "print(\"3D Tensor:\", tensor_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dea97a-29cc-4325-b98b-716304fdc9f5",
   "metadata": {},
   "source": [
    "- __Shape__ describes the number of elements in each dimension of the tensor. For example, a tensor with shape `[2, 3]` has 2 rows and 3 columns.\n",
    "\n",
    "   ```python\n",
    "    print(\"Shape of Matrix:\", matrix.shape)         # Shape: (2, 3)\n",
    "    ```\n",
    "<br/>\n",
    "\n",
    "- Tensors can hold data in __different types__: integers (`tf.int32`, `tf.int64`), floating points (`tf.float32`, `tf.float64`), strings, etc.\n",
    "\n",
    "  ```python\n",
    "    float_tensor = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "    print(\"Float Tensor:\", float_tensor, \"Type:\", float_tensor.dtype)\n",
    "    ```\n",
    "<br/>\n",
    "\n",
    "- TensorFlow supports various tensor __operations__, like\n",
    "    - Addition: `tf.add`\n",
    "    - Multiplication: `tf.multiply`\n",
    "    - Matrix Multiplication: `tf.matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451dc972-537d-4c2c-bb6c-7037c3cc4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise Addition:\n",
      " tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "Element-wise Multiplication:\n",
      " tf.Tensor(\n",
      "[[ 5. 12.]\n",
      " [21. 32.]], shape=(2, 2), dtype=float32)\n",
      "Matrix Multiplication:\n",
      " tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n",
      "Division:\n",
      " tf.Tensor(\n",
      "[[0.2        0.33333334]\n",
      " [0.42857143 0.5       ]], shape=(2, 2), dtype=float32)\n",
      "Sum: tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "Mean: tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "Max: tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "Min: tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Exponentiation:\n",
      " tf.Tensor(\n",
      "[[ 2.7182817  7.389056 ]\n",
      " [20.085537  54.59815  ]], shape=(2, 2), dtype=float32)\n",
      "Square Root:\n",
      " tf.Tensor(\n",
      "[[1.        1.4142135]\n",
      " [1.7320508 2.       ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define tensors with float data type\n",
    "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "# Element-wise operations\n",
    "add_result = tf.add(a, b)\n",
    "multiply_result = tf.multiply(a, b)\n",
    "div_result = tf.divide(a, b)\n",
    "\n",
    "# Matrix multiplication\n",
    "matmul_result = tf.matmul(a, b)\n",
    "\n",
    "# Aggregation operations\n",
    "sum_result = tf.reduce_sum(a)\n",
    "mean_result = tf.reduce_mean(a)\n",
    "max_result = tf.reduce_max(a)\n",
    "min_result = tf.reduce_min(a)\n",
    "\n",
    "# Exponentiation and square root\n",
    "exp_result = tf.exp(a)\n",
    "sqrt_result = tf.sqrt(a)\n",
    "\n",
    "# Print results\n",
    "print(\"Element-wise Addition:\\n\", add_result)\n",
    "print(\"Element-wise Multiplication:\\n\", multiply_result)\n",
    "print(\"Matrix Multiplication:\\n\", matmul_result)\n",
    "print(\"Division:\\n\", div_result)\n",
    "\n",
    "print(\"Sum:\", sum_result)             \n",
    "print(\"Mean:\", mean_result)           \n",
    "print(\"Max:\", max_result)             \n",
    "print(\"Min:\", min_result)\n",
    "\n",
    "print(\"Exponentiation:\\n\", exp_result)\n",
    "print(\"Square Root:\\n\", sqrt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaaafc-1557-4d8e-b9d8-02489fc6b6fa",
   "metadata": {},
   "source": [
    "## `tf.data.Dataset` API\n",
    "It is a powerful tool for building input pipelines to handle large datasets efficiently. It simplifies the process of loading, preprocessing, and iterating over data, making it ideal for preparing data for machine learning models, especially when working with large and complex datasets.\n",
    "\n",
    "### Key Concepts\n",
    "1. **Data Representation**:\n",
    "    - The `Dataset` API represents a sequence of data elements, where each element consists of one or more components. Components can be single values, vectors, or even complex data structures like images and labels.\n",
    "    - This API allows users to create datasets from arrays, files, or generators and transform them with functions that prepare the data for machine learning models.\n",
    "\n",
    "2. **Creating a Dataset**:\n",
    "    - You can create datasets from different sources such as in-memory data (e.g., lists, arrays), TFRecord files, or text files.\n",
    "    - The `Dataset.from_tensor_slices()` method is commonly used to create datasets from in-memory data, such as lists or NumPy arrays.\n",
    "\n",
    "3. **Transformations**: The API provides various transformations to process data efficiently, such as\n",
    "    - `map()`: Apply a function to each element in the dataset.\n",
    "    - `batch()`: Combines consecutive elements into batches, enabling mini-batch processing.\n",
    "    - `shuffle()`: Randomly shuffles the data to improve model training and reduce overfitting.\n",
    "    - `repeat()`: Repeats the dataset multiple times, useful for training over multiple epochs.\n",
    "    \n",
    "4. **Optimizing Pipelines**:\n",
    "    - The API includes optimizations like prefetching, parallel mapping, and caching.\n",
    "    - `prefetch()` helps overlap data preparation with model training by fetching the next -batch while the current batch is being processed.\n",
    "    - `cache()` can store the dataset in memory after the first epoch, speeding up subsequent epochs.\n",
    "\n",
    "5. **Iteration**: A `tf.data.Dataset` object is iterable, allowing you to loop through it directly or retrieve individual batches in a session or eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe39a21-1c80-4c8e-8301-7f8d5b9b321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [[2.8997488 3.859724 ]\n",
      " [4.8605475 6.011254 ]]\n",
      "Labels: [1 0]\n",
      "Features: [[7.075528   7.8318596 ]\n",
      " [0.91776663 1.8035622 ]]\n",
      "Labels: [1 0]\n",
      "Features: [[5.1282496 6.0109253]\n",
      " [3.060341  3.939802 ]]\n",
      "Labels: [0 1]\n",
      "Features: [[6.930897  7.861804 ]\n",
      " [1.2034491 2.0419757]]\n",
      "Labels: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: features (inputs) and labels (targets)\n",
    "features = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]], dtype=np.float32)\n",
    "labels = np.array([0, 1, 0, 1], dtype=np.int32)\n",
    "\n",
    "# Step 1: Create a Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Step 2: Shuffle, Batch, and Repeat\n",
    "batch_size = 2\n",
    "dataset = dataset.shuffle(buffer_size=4).batch(batch_size).repeat(2)  # Shuffle, batch, and repeat\n",
    "\n",
    "# Step 3: Map Transformation (Optional)\n",
    "# Apply a simple map transformation to add noise to features (e.g., for data augmentation)\n",
    "def add_noise(features, labels):\n",
    "    noise = tf.random.normal(shape=tf.shape(features), mean=0.0, stddev=0.1)\n",
    "    features = features + noise\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset.map(add_noise)\n",
    "\n",
    "# Step 4: Prefetch for Optimized Performance\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Step 5: Iterate over the Dataset\n",
    "for batch in dataset:\n",
    "    print(\"Features:\", batch[0].numpy())\n",
    "    print(\"Labels:\", batch[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ddcdd-724b-4954-a068-ceefb8eefdd7",
   "metadata": {},
   "source": [
    "# Building and Training Models with Keras API\n",
    "## Sequential API\n",
    "The `Sequential` API in Keras is an easy and intuitive way to build and train simple neural network models layer by layer. This API is ideal for models that consist of a single input and a single output and follow a linear stack of layers.\n",
    "### Key Concepts\n",
    "1. **Sequential Model**:\n",
    "    - The `Sequential` model is a linear stack of layers that are added one after another.\n",
    "    - It is particularly useful for straightforward models like feedforward neural networks (dense layers) and simple convolutional or recurrent layers.\n",
    "2. Layer Stacking:\n",
    "    - You define the model by stacking different types of layers in the order you want them to execute.\n",
    "    - Layers include Dense (fully connected layers), Conv2D (convolutional layers), Flatten, Dropout, and more.\n",
    "3. **Compilation**:\n",
    "    - After defining the layers, you compile the model to configure the learning process. This involves setting the optimizer, loss function, and metrics.\n",
    "    - The optimizer (e.g., `adam`, `sgd`) controls how the model learns.\n",
    "    - The loss function (e.g., `binary_crossentropy`, `mean_squared_error`) measures the model’s error and guides the training.\n",
    "    - Metrics (e.g., `accuracy`) are used to evaluate the model’s performance.\n",
    "4. **Training the Model**:\n",
    "    - After compiling, the model can be trained on data using the `fit()` method. This involves passing the input data, target labels, batch size, and number of epochs.\n",
    "    - During training, the model optimizes its weights to reduce the loss function value.\n",
    "5. **Evaluation and Prediction**:\n",
    "    - Once trained, the model’s performance can be evaluated on a test set using `evaluate()`.\n",
    "    - Predictions for new data can be made using `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f5d247-b211-4cb0-a667-df9ed7b82b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python3.11\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5280570983886719\n",
      "Training Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "Predictions: [[0.4936982 ]\n",
      " [0.41783324]\n",
      " [0.8179251 ]\n",
      " [0.50176513]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Step 1: Define a Sequential Model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(2,)),  # Input layer with 2 features, 16 neurons, ReLU activation\n",
    "    Dense(8, activation='relu'),                     # Hidden layer with 8 neurons, ReLU activation\n",
    "    Dense(1, activation='sigmoid')                   # Output layer with 1 neuron, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Step 2: Compile the Model\n",
    "model.compile(optimizer='adam',                     # Adam optimizer\n",
    "              loss='binary_crossentropy',           # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])                 # Metric to monitor accuracy during training\n",
    "\n",
    "# Sample training data (2D points)\n",
    "import numpy as np\n",
    "X_train = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Step 3: Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)  # Training the model for 100 epochs\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training Loss:\", loss)\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "predictions = model.predict(X_train)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
