{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23dfb411-9431-4654-9cb1-aac8c4f89175",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It models decisions in a tree-like structure, where internal nodes represent features, branches represent decision rules, and leaf nodes represent outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59fb73-840e-453a-b6e3-3a70a383ae16",
   "metadata": {},
   "source": [
    "# Structure of a Decision Tree\n",
    "- __Root Node:__ The topmost node in a decision tree, representing the entire dataset. It splits into two or more branches.\n",
    "- __Decision Nodes:__ These are nodes that make decisions and split the data further based on certain conditions.\n",
    "- __Leaf Nodes:__ These nodes do not split further and contain the outcome or prediction of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584182b-fbee-433d-948c-e8d269c9f61d",
   "metadata": {},
   "source": [
    "# How Decision Trees Work\n",
    "A decision tree uses a divide and conquer approach:\n",
    "- It selects a feature to split the data based on a criterion (like Outlook, Humidity in your PlayTennis dataset).\n",
    "- The goal is to choose the feature that best separates the classes or reduces error in predictions.\n",
    "- This splitting process continues recursively until a stopping condition is met, like a maximum tree depth, minimum number of samples per node, or when further splitting does not add any information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee3e5e-2d3e-4984-9e9c-c548812fd408",
   "metadata": {},
   "source": [
    "# Splitting Criteria\n",
    "\n",
    "## Entropy (Information Gain) (for Classification)\n",
    "Measures the uncertainty in a node. Information Gain is calculated as the difference in entropy before and after a split.\n",
    "\n",
    "__Formula:__\n",
    "\n",
    "\\$\n",
    "Entropy = -\\sum_{i=1}^{n} p_i \\log_2 (p_i)\n",
    "\\$\n",
    "\n",
    "where \\$ p_i \\$ is the proportion of samples belonging to class \\$ i \\$.\n",
    "\n",
    "Information Gain helps to select the attribute that reduces uncertainty the most.\n",
    "\n",
    "## Gini Impurity (for Classification)\n",
    "Measures the impurity or likelihood of a random sample being incorrectly classified if it were randomly assigned a label according to the distribution of labels in the node.\n",
    "\n",
    "__Formula:__\n",
    "\n",
    "\\$\n",
    "Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "\\$\n",
    "\n",
    "where \\$ p_i \\$ is the proportion of samples belonging to class \\$ i \\$.\n",
    "\n",
    "A Gini value of 0 indicates perfect purity (all samples in the node belong to one class).\n",
    "\n",
    "## Conceptual Differences\n",
    "### Entropy\n",
    "- Measures the amount of uncertainty or randomness in the dataset.\n",
    "- Ranges from 0 (pure set) to 1 (maximum uncertainty for a binary split).\n",
    "- It is based on the concept of information gain, which measures how much information a feature provides in reducing uncertainty.\n",
    "\n",
    "### Gini Impurity\n",
    "- Measures how often a randomly chosen element would be incorrectly classified.\n",
    "- Ranges from 0 (pure set) to 0.5 (most mixed for binary classes).\n",
    "- It is computationally simpler and faster to calculate compared to entropy.\n",
    "\n",
    "## When to Choose Gini Impurity\n",
    "- Gini impurity is `faster` to compute compared to entropy because it `doesn't involve logarithmic` calculations.\n",
    "- When dealing with large datasets\n",
    "- For binary classification tasks, Gini impurity is often preferred because it directly measures the likelihood of a misclassification based on the split.\n",
    "\n",
    "## When to Choose Entropy (Information Gain)\n",
    "- Preferred when you want a more theoretically grounded approach to understanding how your tree is making decisions.\n",
    "- Entropy can lead to splits that are more balanced, as it measures the overall randomness in the dataset.\n",
    "- If you have a dataset with a high degree of class imbalance, entropy might help create more informative splits.\n",
    "- When you want to account for the overall uncertainty in your dataset, entropy can be more suitable. It takes into account the entire distribution of classes rather than just misclassification rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d867e57-998b-44c3-9a6b-9e75a16e4ef8",
   "metadata": {},
   "source": [
    "# Build the Decision Tree with Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e431d-435b-4251-a408-b774db04c387",
   "metadata": {},
   "source": [
    "## 1. Choose the Target Attribute\n",
    "The target attribute represents the outcome or decision that the model aims to predict.\n",
    "\n",
    "`Outlook`, `Temperature`, `Humidity`, and `Wind` are features, while `PlayTennis` (`Yes` or `No`) is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339e55d-19d9-44c5-8aef-bc9425f824c6",
   "metadata": {},
   "source": [
    "## 2.Calculate the Entropy of the Entire Dataset\n",
    "\n",
    "Entropy measures the impurity or randomness in the dataset. For a binary classification, the formula for entropy \\( H \\) is:\n",
    "\n",
    "\\$\n",
    "H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)\n",
    "\\$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\$ S \\$: The set of instances (data).\n",
    "- \\$ p_+ \\$: The proportion of positive instances (e.g., \"Yes\" for `PlayTennis`).\n",
    "- \\$ p_- \\$: The proportion of negative instances (e.g., \"No\" for `PlayTennis`).\n",
    "\n",
    "__Example:__\n",
    "\\$ Entropy(S) = - \\frac{14}{9} \\log_2 \\left( \\frac{14}{9} \\right) - \\frac{14}{5} \\log_2 \\left( \\frac{14}{5} \\right) \\approx 0.940\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a999368-8a93-49ac-b725-f1ab2e6d1225",
   "metadata": {},
   "source": [
    "## 3.1 Calculate Entropy for Each Feature's Possible Splits\n",
    "For each feature, calculate the entropy of splitting the dataset based on its different values.\n",
    "\n",
    "__Example:__\n",
    "\n",
    "Let's calculate for `Outlook`, which has three values: `Sunny`, `Overcast`, and `Rain`.\n",
    "\n",
    "\\$\n",
    "Entropy(Sunny) = - \\frac{2}{5} \\log_2 \\left( \\frac{2}{5} \\right) - \\frac{3}{5} \\log_2 \\left( \\frac{3}{5} \\right) \\approx 0.971\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Entropy(Overcast) = - \\frac{4}{4} \\log_2 \\left( \\frac{4}{4} \\right) = 0\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Entropy(Rain) = - \\frac{3}{5} \\log_2 \\left( \\frac{3}{5} \\right) - \\frac{2}{5} \\log_2 \\left( \\frac{2}{5} \\right) \\approx 0.971\n",
    "\\$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b676557-7ed9-466c-9f29-f854a119f5f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Calculate the Weighted Entropy of Each Split\n",
    "Use the weighted average of the entropies to calculate the entropy after the split\n",
    "\n",
    "__Example:__\n",
    "\n",
    "\\$\n",
    "Entropy(S, Outlook) = \\frac{5}{14} \\cdot 0.971 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14} \\cdot 0.971 \\approx 0.693\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e1cba-17b2-4140-9a1e-4baefeb9f585",
   "metadata": {},
   "source": [
    "## 3.3 Calculate Information Gain for Each Feature\n",
    "Information Gain measures the reduction in entropy after a split\n",
    "\n",
    "__Example:__\n",
    "\n",
    "\\$\n",
    "Information Gain(S,Outlook)=0.940−0.693=0.247\n",
    "\\$\n",
    "\\$\n",
    "Information Gain(S,Temperature)=0.940−0.911=0.029\n",
    "\\$\n",
    "\\$\n",
    "Information Gain(S,Humidity)=0.940−0.789=0.151\n",
    "\\$\n",
    "\\$\n",
    "Information Gain(S,Wind)=0.940−0.892=0.048\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71091cc-8047-47f1-80d6-d560d09af582",
   "metadata": {},
   "source": [
    "## 3.4 Choose the Feature with the Highest Information Gain\n",
    "Select the feature with the highest information gain as the root node of the decision tree.\n",
    "\n",
    "__Example:__ `Outlook` has the highest gain (0.247), so it becomes the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef43a1-0441-456d-a890-3449b756e548",
   "metadata": {},
   "source": [
    "## 3.5 Split the Dataset Based on the Chosen Feature\n",
    "Divide the dataset into subsets based on the selected feature (`Outlook`):\n",
    "- Subset 1: `Outlook = Sunny`\n",
    "- Subset 2: `Outlook = Overcast`\n",
    "- Subset 3: `Outlook = Rain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23199a12-a37b-4f3a-88c4-5f3130ff4765",
   "metadata": {},
   "source": [
    "## 3.6 Repeat the Process for Each Subset\n",
    "- For each subset, calculate the entropy and information gain for the remaining features (`Temperature`, `Humidity`, `Wind`).\n",
    "- Choose the feature with the highest information gain within each subset and split again.\n",
    "- Continue until you meet one of the stopping criteria:\n",
    "    - The subset is pure (all instances have the same class).\n",
    "    - No further information gain is achieved (i.e., all remaining features result in 0 information gain).\n",
    "    - A maximum depth of the tree is reached (to avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc919f0-0c42-4562-a3b2-5ffdc576e513",
   "metadata": {},
   "source": [
    "# 4. Create Leaf Nodes\n",
    "- For each pure subset (where all instances have the same target value), create a leaf node.\n",
    "- Assign the majority class in the subset as the class for that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641520e-6027-40c0-bfd0-4ecded04e0fc",
   "metadata": {},
   "source": [
    "# 5. Construct the Decision Tree\n",
    "- Combine all nodes and branches into a decision tree structure.\n",
    "- At each node, the selected feature determines the next branch to follow based on its values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a055a5-e86a-4a4f-adde-3152fbcfde1f",
   "metadata": {},
   "source": [
    "# Build Decision Tree with Gini Index\n",
    "\n",
    "For a binary classification problem with classes \\$ Yes \\$ and \\$ No \\$, the formula becomes:\n",
    "\n",
    "\\$\n",
    "Gini(S) = 1 - (p_1^2 + p_2^2)\n",
    "\\$\n",
    "\n",
    "where \\$ p_1 \\$ is the proportion of Yes instances and \\$ p_2 \\$ is the proportion of No instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d710010-5cf0-44ab-bb52-2ca5d2b9e01b",
   "metadata": {},
   "source": [
    "## 1. Choose the Target Attribute\n",
    "The target attribute represents the outcome or decision that the model aims to predict.\n",
    "\n",
    "`Outlook`, `Temperature`, `Humidity`, and `Wind` are features, while `PlayTennis` (`Yes` or `No`) is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878b426-9c86-485e-ac53-38bcfca056dd",
   "metadata": {},
   "source": [
    "## 2.Calculate the Gini Impurity of the Entire Dataset\n",
    "\\$\n",
    "Gini(S) = 1 - \\left( \\left( \\frac{14}{9} \\right)^2 + \\left( \\frac{14}{5} \\right)^2 \\right) \\approx 0.337\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60e679-2a65-474e-9964-4acc442c395e",
   "metadata": {},
   "source": [
    "### 3.1 Calculate the Gini Impurity for Each Attribute\n",
    "\n",
    "For each feature, calculate the entropy of splitting the dataset based on its different values.\n",
    "\n",
    "__Example:__\n",
    "\n",
    "Let's calculate for `Outlook`, which has three values: `Sunny`, `Overcast`, and `Rain`.\n",
    "\n",
    "\\$\n",
    "Gini(Sunny) = 1 - \\left( \\left( \\frac{5}{2} \\right)^2 + \\left( \\frac{5}{3} \\right)^2 \\right) \\approx = 0.48\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Gini(Overcast) = 1 - \\left( \\left( \\frac{1}{2} \\right)^2 + 0^2 \\right) = 0\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Gini(Rain) = 1 - \\left( \\left( \\frac{5}{3} \\right)^2 + \\left( \\frac{5}{2} \\right)^2 \\right) \\approx = 0.48\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c1a24-0136-4289-8784-67aabfb3704a",
   "metadata": {},
   "source": [
    "### 3.2 Calculate the Weighted Gini of Each Split\n",
    "Use the weighted average of the ginis to calculate the gini after the split\n",
    "\n",
    "\\$\n",
    "Gini(S, Outlook) = \\frac{14}{5} \\cdot 0.48 + \\frac{14}{4} \\cdot 0 + \\frac{14}{5} \\cdot 0.48 \\approx  0.343\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Gini(S, Temperature) = \\frac{14}{4} \\cdot 0.5 + \\frac{14}{6} \\cdot 0.444 + \\frac{14}{4} \\cdot 0.375 \\approx 0.437\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Gini(S, Humidity) = \\frac{14}{7} \\cdot 0.49 + \\frac{14}{7} \\cdot 0.245 \\approx 0.245\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "Gini(S, Wind) = \\frac{14}{8} \\cdot 0.375 + \\frac{14}{6} \\cdot 0.5 \\approx 0.428\n",
    "\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b14d0-47db-44aa-8070-fe1c5c79681c",
   "metadata": {},
   "source": [
    "### 3.3 Choose the Attribute with the Lowest Gini Impurity\n",
    "\n",
    "Select the feature with the lowest weighted Gini impurity as the root node of the decision tree.\n",
    "\n",
    "__Example:__ `Outlook` has the lowest gini (0.343), so it becomes the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27178c1d-0d0a-4246-8037-61053f1457fe",
   "metadata": {},
   "source": [
    "### 3.4 Split the Dataset Based on the Chosen Feature\n",
    "Divide the dataset into subsets based on the selected feature (`Outlook`):\n",
    "- Subset 1: `Outlook = Sunny`\n",
    "- Subset 2: `Outlook = Overcast`\n",
    "- Subset 3: `Outlook = Rain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ff3d8-2bc7-46b6-8f72-8fdc828a34c2",
   "metadata": {},
   "source": [
    "## 3.5 Repeat the Process for Each Subset\n",
    "- For each subset, calculate the gini for the remaining features (`Temperature`, `Humidity`, `Wind`).\n",
    "- Choose the feature with the lowest gini within each subset and split again.\n",
    "- Continue until you meet one of the stopping criteria:\n",
    "    - The subset is pure (all instances have the same class).\n",
    "    - No further gini is achieved (i.e., all remaining features result in 0 information gain).\n",
    "    - A maximum depth of the tree is reached (to avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da2756-7cd1-4550-9361-22c92bb5d726",
   "metadata": {},
   "source": [
    "# 4. Create Leaf Nodes\n",
    "- For each pure subset (where all instances have the same target value), create a leaf node.\n",
    "- Assign the majority class in the subset as the class for that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662aa9e-4664-489f-ad6d-a3f3c8a3c282",
   "metadata": {},
   "source": [
    "# 5. Construct the Decision Tree\n",
    "- Combine all nodes and branches into a decision tree structure.\n",
    "- At each node, the selected feature determines the next branch to follow based on its values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
